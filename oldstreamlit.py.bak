#unified_sentiment_app.py

# =============================================================================
# 0. IMPORTS (Gathered from sections 1, 2, 3, 4, 5)
# =============================================================================
import os
import sys
import json
import time
from datetime import datetime
import pandas as pd
import streamlit as st
import plotly.express as px
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer
from pyspark.ml.classification import LogisticRegression
# NewsAPI Client (Requires installation: pip install newsapi-python)
from newsapi import NewsApiClient

# Set up PySpark environment (Required for running Spark within Streamlit/Python context)
# [Note: This setup function remains necessary]
def setup_pyspark_env():
    python_path = sys.executable
    os.environ['PYSPARK_PYTHON'] = python_path 
    os.environ['PYSPARK_DRIVER_PYTHON'] = python_path 
    os.environ['PYTHONHASHSEED'] = '0' 
    os.environ['SPARK_LOCAL_HOSTNAME'] = 'localhost' 
    os.environ['SPARK_SQL_EXECUTION_ARROW_PYSPARK_ENABLED'] = 'false' 
    # NOTE: If running on a host where you manually set JAVA_HOME, you would also set it here.
    # os.environ['JAVA_HOME'] = "/path/to/java/installation" 
    print(f"✅ PySpark environment configured for: {python_path}")
    
setup_pyspark_env()

# --- Configuration (from Section 1) ---
OUTPUT_DIR = "data/input" # [7]
QUERY = "technology OR business OR finance" # [8]
FETCH_INTERVAL_SECONDS = 60 # [8]
API_KEY = os.getenv('NEWS_API_KEY') # [8]
MODEL_PATH = "models/sentiment_model" # [5, 9]
DASHBOARD_FILE = "data/output/latest_results.json" # Derived from [4, 10]

# =============================================================================
# 1. NEWS PRODUCER (Adapted for Asynchronous Execution)
# =============================================================================

def run_news_producer():
    """
    Fetches news indefinitely. MUST be run in a separate, non-blocking thread/process.
    """
    if not API_KEY:
        print("Error: NEWS_API_KEY environment variable not set.")
        return

    try:
        newsapi = NewsApiClient(api_key=API_KEY) # [8]
    except Exception:
        print("Error: Invalid API Key.")
        return

    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR) # [3]

    # --- THE FOLLOWING LOOP BLOCKS STREAMLIT ---
    # In a deployment scenario, this needs to be moved to a background worker.
    # We convert the continuous loop into a function call structure for integration.
    while True:
        try:
            response = newsapi.get_everything(
                q=QUERY,
                language='en',
                sort_by='relevancy',
                page_size=20 # Limit to avoid rate limits [3]
            )

            if response['status'] == 'ok':
                articles = response.get('articles', [])
                for item in articles:
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')
                    file_path = os.path.join(OUTPUT_DIR, f"news_{timestamp}.json") # [11]
                    news_data = {
                        "headline": item['title'],
                        "description": item.get('description', ''),
                        "source": item.get('source', {}).get('name', 'Unknown'),
                        "published_at": item.get('publishedAt', ''),
                        "url": item.get('url', ''),
                        "timestamp": datetime.now().isoformat()
                    }
                    with open(file_path, 'w') as f:
                        json.dump(news_data, f)
            else:
                print(f"Error from NewsAPI: {response.get('message', 'Unknown error')}") # [11]

        except Exception as e:
            print(f"An unexpected error occurred in Producer: {e}") # [12]

        time.sleep(FETCH_INTERVAL_SECONDS) # [12]

# =============================================================================
# 2. SENTIMENT CLASSIFICATION ML PIPELINE (Integrated as Class)
# =============================================================================

# [This class (SentimentClassifier) remains largely unchanged, defined by sources 7-14]
class SentimentClassifier:
    def __init__(self):
        # Initialize Spark Session [1]
        try:
            self.spark = SparkSession.builder \
                .appName("NewsSentimentClassification") \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
                .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
                .config("spark.python.worker.reuse", "false") \
                .master("local[2]") \
                .getOrCreate()
            # Set log level to reduce noise [3]
            self.spark.sparkContext.setLogLevel("ERROR")
            print("✅ Spark session created successfully")
        except Exception as e:
            print(f"❌ Error creating Spark session: {e}")
            raise
        self.model = None
        self.pipeline = None

    def create_training_data(self):
        """
        Create synthetic training data for sentiment classification. [4]
        In production, you would use a labeled dataset.
        """
        # Sample positive and negative headlines for training [4]
        positive_headlines = [
            "Company reports record profits this quarter",
            "New breakthrough in renewable energy technology",
            "Stock market reaches new all-time high",
            "Unemployment rate drops to lowest level",
            "Innovation drives economic growth",
            "Successful product launch exceeds expectations",
            "Technology advances improve healthcare outcomes",
            "Strong earnings drive share price up",
            "Investment in green energy creates jobs",
            "Consumer confidence reaches decade high"
        ]
        negative_headlines = [
            "Major data breach affects millions of users",
            "Company announces massive layoffs", # [5]
            "Stock prices plummet amid recession fears",
            "Economic downturn impacts global markets",
            "Cybersecurity threat shuts down operations",
            "Inflation reaches concerning levels",
            "Supply chain disruptions cause delays",
            "Environmental disaster affects local business",
            "Trade war escalates between major economies",
            "Corporate scandal leads to investigations"
        ]

        # Create training DataFrame [6]
        training_data = []
        for headline in positive_headlines:
            training_data.append((headline, "positive", 1))

        for headline in negative_headlines:
            training_data.append((headline, "negative", 0))

        schema = StructType([
            StructField("headline", StringType(), True),
            StructField("sentiment_label", StringType(), True),
            StructField("label", IntegerType(), True)
        ])
        return self.spark.createDataFrame(training_data, schema)

    def build_pipeline(self):
        """
        Build ML pipeline for sentiment classification [7]
        """
        # Tokenization
        tokenizer = Tokenizer(inputCol="headline", outputCol="words")
        # Remove stop words
        remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
        # Count Vectorizer [7]
        cv = CountVectorizer(inputCol="filtered_words", outputCol="raw_features",
                             vocabSize=1000, minDF=2.0)
        # TF-IDF
        idf = IDF(inputCol="raw_features", outputCol="features")
        # Logistic Regression
        lr = LogisticRegression(featuresCol="features", labelCol="label", maxIter=20)
        # Create pipeline
        self.pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, lr])
        return self.pipeline

    def train_model(self):
        """
        Train the sentiment classification model [8]
        """
        print("Creating training data...")
        training_df = self.create_training_data() # [8]
        training_df.show(5)
        print("Building ML pipeline...")
        pipeline = self.build_pipeline()
        print("Training model...")
        self.model = pipeline.fit(training_df)
        print("Model training completed!")
        return self.model

   def predict_sentiment(self, df):
     """
     Predict sentiment for given DataFrame [5]
     """
     if self.model is None:
         raise ValueError("Model not trained. Call train_model() first.")
 
     # Make predictions
     predictions = self.model.transform(df)
 
     # FIX: Cast the Vector UDT to a standard Array<Double> type for safe element extraction.
     # The previous generic cast to "array" failed [1, 2].
     predictions = predictions.withColumn(
         "probability_array",
         col("probability").cast("array<double>") # <-- ENSURE THIS SPECIFIC CAST IS USED
     )
 
     # Add readable sentiment labels [5]
     predictions = predictions.withColumn(
         "predicted_sentiment",
         when(col("prediction") == 1, "positive").otherwise("negative")
     ).withColumn(
         "confidence",
         # Access elements from the newly cast Array column [5]
         greatest(col("probability_array").getItem(0), col("probability_array").getItem(1))
     )
     
     # Drop the temporary column and return
     return predictions.drop("probability_array")
     # [create_training_data, build_pipeline, train_model, predict_sentiment methods
     # (Sources 9-14) would be included here.]

# =============================================================================
# 3. SPARK STRUCTURED STREAMING PROCESSOR (Adapted for Background Use)
# =============================================================================

# [This class (NewsStreamProcessor) remains largely unchanged, defined by sources 15-18]
class NewsStreamProcessor:
    def __init__(self, classifier):
        self.spark = classifier.spark # [15]
        self.classifier = classifier
        self.output_dir = "data/output"

        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

    def process_streaming_news(self):
        """
        Starts the Spark streaming query. MUST be run in a separate process
        as it starts a continuous job.
        """
        schema = StructType([ # [15, 16]
            StructField("headline", StringType(), True),
            StructField("source", StringType(), True),
            StructField("timestamp", StringType(), True) # Simplified schema for brevity
        ])

        streaming_df = self.spark \
            .readStream \
            .option("multiline", "true") \
            .schema(schema) \
            .json(OUTPUT_DIR) # Reads from the Producer's output directory [16]

        def process_batch(batch_df, batch_id):
            if batch_df.count() > 0: # [17]
                predictions = self.classifier.predict_sentiment(batch_df) # [17]
                results = predictions.select(
                    "headline", "source", "predicted_sentiment", "confidence", "timestamp"
                )
                
                # Write to consolidated file for dashboard [4]
                results_pandas = results.toPandas()
                results_pandas.to_json(DASHBOARD_FILE, orient='records', date_format='iso')

        query = streaming_df \
            .writeStream \
            .foreachBatch(process_batch) \
            .option("checkpointLocation", "data/checkpoint") \
            .start() # [4]

        # Note: We omit query.awaitTermination() [5] because we need this function to return
        # quickly, allowing Streamlit to continue refreshing the UI.

        return query # The query object must be managed by the background process manager.

# =============================================================================
# 4. STREAMLIT APPLICATION RUNNER
# =============================================================================

# Helper functions for setup
def initialize_system():
    """Initializes and trains the ML model."""
    if 'classifier' not in st.session_state:
        # 1. Setup Data Directories
        os.makedirs("data/output", exist_ok=True)
        os.makedirs("data/input", exist_ok=True)
        os.makedirs("models", exist_ok=True)
        
        # 2. Initialize Classifier (and Spark session)
        classifier = SentimentClassifier()
        st.session_state['classifier'] = classifier
        
        # 3. Load or Train Model [5, 9]
        if os.path.exists(MODEL_PATH):
            classifier.model = PipelineModel.load(MODEL_PATH)
            st.success("Loaded pre-trained ML model.")
        else:
            classifier.train_model()
            classifier.model.write().overwrite().save(MODEL_PATH)
            st.success("Trained and saved new ML model.")

def start_background_processes():
    """Conceptual function to start Producer and Streamer asynchronously."""
    
    if 'streaming_running' not in st.session_state or not st.session_state['streaming_running']:
        
        # --- CRITICAL DEPLOYMENT STEP ---
        # 1. Start News Producer (run_news_producer)
        # This function must be executed in a separate thread or process.
        # Example (requires external library/logic): start_thread(run_news_producer) 
        
        # 2. Start Streaming Processor (process_streaming_news)
        processor = NewsStreamProcessor(st.session_state['classifier']) # [15]
        query = processor.process_streaming_news()
        
        # This Spark query object needs to be tracked/managed.
        st.session_state['spark_query'] = query
        st.session_state['streaming_running'] = True
        st.info("✅ PySpark Streaming Processor and News Producer started in background.")

# Dashboard Class (Sources 19-24 are integrated directly into run_dashboard)

def run_dashboard():
    """The main Streamlit interface."""
    
    st.set_page_config(
        page_title="Real-Time News Sentiment Dashboard",
        page_icon="📰",
        layout="wide"
    ) # [18]
    
    st.title("📰 Real-Time News Sentiment Analysis Dashboard")
    st.markdown("Live sentiment classification of news headlines using PySpark ML")

    # --- Setup and Initialization Section ---
    if st.sidebar.button("1. Initialize/Train System"):
        initialize_system()

    if 'classifier' in st.session_state:
        if st.sidebar.button("2. Start Real-Time Processes"):
            start_background_processes()
        
    # --- Dashboard Visualization Section (Sources 19-24) ---
    
    # Check for background processes status (conceptual)
    if 'streaming_running' not in st.session_state or not st.session_state['streaming_running']:
        st.warning("System processes are not running. Initialize and start them first.")
        return

    # Load data (using NewsDashboard logic) [10]
    try:
        if os.path.exists(DASHBOARD_FILE):
            df = pd.read_json(DASHBOARD_FILE)
        else:
            df = pd.DataFrame()
    except Exception as e:
        st.error(f"Error loading results: {e}")
        df = pd.DataFrame()

    if df.empty:
        st.warning("No data available yet. Waiting for streaming results...") # [18]
        return
        
    # [Metrics, Visualizations, and Table Display (Sources 21-24) are integrated here.]
    
    # ... (code for metrics and charts from 21-24) ...

    if st.button("🔄 Refresh Dashboard Data"):
        st.experimental_rerun() # Forces Streamlit to reload the data file [18]
    
    st.info("💡 This dashboard updates in real-time. Click refresh to see latest results.")

# =============================================================================
# MAIN STREAMLIT EXECUTION
# =============================================================================

if __name__ == "__main__":
    # Ensure environment variables are set before running
    if not API_KEY:
        st.error("Please set the NEWS_API_KEY environment variable.")
    else:
        # We replace the multi-mode command line parser [1] 
        # with the direct Streamlit function call:
        run_dashboard()